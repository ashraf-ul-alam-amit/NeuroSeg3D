{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ed68f29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.8.0\n",
      "2.8.0\n",
      "Segmentation Models: using `tf.keras` framework.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "print(tf.__version__)\n",
    "print(keras.__version__)\n",
    "\n",
    "from tensorflow.keras.utils import normalize\n",
    "import os\n",
    "import glob\n",
    "import cv2\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler, ModelCheckpoint, EarlyStopping, TensorBoard, ReduceLROnPlateau, CSVLogger\n",
    "from tensorflow.keras import models, layers, regularizers\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "from Codes.custom_datagen import imageLoader,load_img\n",
    "import random\n",
    "\n",
    "import segmentation_models_3D as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9278030d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Dice coefficient function\n",
    "def dice_coef(y_true, y_pred):\n",
    "    smooth = 1e-15\n",
    "    intersection = tf.reduce_sum(y_true * y_pred)\n",
    "    return (2. * intersection + smooth) / (tf.reduce_sum(y_true) + tf.reduce_sum(y_pred) + smooth)\n",
    "\n",
    "# Define the Dice coefficient loss function\n",
    "def dice_coef_loss(y_true, y_pred):\n",
    "    return 1 - dice_coef(y_true, y_pred)  # Use 1 - Dice coefficient as the loss\n",
    "\n",
    "\n",
    "def jacard_coef(y_true, y_pred):\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = K.sum(y_true_f * y_pred_f)\n",
    "    return (intersection + 1.0) / (K.sum(y_true_f) + K.sum(y_pred_f) - intersection + 1.0)\n",
    "\n",
    "\n",
    "def jacard_coef_loss(y_true, y_pred):\n",
    "    return -jacard_coef(y_true, y_pred)\n",
    "\n",
    "def dice_coef(y_true, y_pred):\n",
    "    smooth = 1e-15\n",
    "    intersection = tf.reduce_sum(y_true * y_pred)\n",
    "    return (2. * intersection + smooth) / (tf.reduce_sum(y_true) + tf.reduce_sum(y_pred) + smooth)\n",
    "\n",
    "def iou(y_true, y_pred):\n",
    "    intersection = tf.reduce_sum(y_true * y_pred)\n",
    "    union = tf.reduce_sum(y_true) + tf.reduce_sum(y_pred) - intersection\n",
    "    return (intersection + 1e-15) / (union + 1e-15)\n",
    "\n",
    "# def dice_coef_loss(y_true, y_pred):\n",
    "#     return -dice_coef(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2d43d9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "model = load_model('training/SP/brats_3d_full_sp.hdf5', custom_objects={'dice_coef_loss': dice_coef_loss, 'dice_coef': dice_coef, 'iou':iou})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a3a7581c",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_img_dir = \"D:/1803070/Brats/Splited_Combined/val/images/\"\n",
    "val_mask_dir = \"D:/1803070/Brats/Splited_Combined/val/masks/\"\n",
    "\n",
    "test_img_dir = \"D:/1803070/Brats/Splited_Combined/test/images/\"\n",
    "test_mask_dir = \"D:/1803070/Brats/Splited_Combined/test/masks/\"\n",
    "\n",
    "val_img_list=os.listdir(val_img_dir)\n",
    "val_mask_list = os.listdir(val_mask_dir)\n",
    "\n",
    "test_img_list=os.listdir(test_img_dir)\n",
    "test_mask_list = os.listdir(test_mask_dir)\n",
    "\n",
    "batch_size = 2\n",
    "\n",
    "val_img_datagen = imageLoader(val_img_dir, val_img_list, \n",
    "                                val_mask_dir, val_mask_list, batch_size)\n",
    "\n",
    "test_img_datagen = imageLoader(test_img_dir, test_img_list, \n",
    "                                test_mask_dir, test_mask_list, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f4d41467",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Dice Score on Validation Data: 0.9932529330253601\n",
      "Average IoU on Validation Data: 0.9866235852241516\n",
      "\n",
      "Average Dice Score on Test Data: 0.9896089434623718\n",
      "Average IoU on Test Data: 0.9796287417411804\n"
     ]
    }
   ],
   "source": [
    "def calculate_metrics(model, data_generator, steps):\n",
    "    dice_scores = []\n",
    "    iou_scores = []\n",
    "    \n",
    "    for _ in range(steps):\n",
    "        X_val, Y_val = next(data_generator)\n",
    "        predictions = model.predict(X_val)\n",
    "        \n",
    "#         # Calculate dice score and IoU for each sample in the batch\n",
    "#         for i in range(len(X_val)):\n",
    "#             dice = dice_coef(Y_val[i], predictions[i])\n",
    "#             i = iou(Y_val[i], predictions[i])\n",
    "#             dice_scores.append(dice)\n",
    "#             iou_scores.append(i)\n",
    "\n",
    "\n",
    "        # Calculate dice score and IoU for whole batch\n",
    "        dice = dice_coef(Y_val, predictions)\n",
    "        i = iou(Y_val, predictions)\n",
    "        dice_scores.append(dice)\n",
    "        iou_scores.append(i)\n",
    "    \n",
    "    # Calculate average scores\n",
    "    avg_dice = sum(dice_scores) / len(dice_scores)\n",
    "    avg_iou = sum(iou_scores) / len(iou_scores)\n",
    "    \n",
    "    return avg_dice, avg_iou\n",
    "\n",
    "# Assuming you want to evaluate on a certain number of steps/batches\n",
    "num_val_steps = 10 \n",
    "\n",
    "# Calculate metrics on validation data\n",
    "avg_val_dice, avg_val_iou = calculate_metrics(model, val_img_datagen, num_val_steps)\n",
    "\n",
    "print(f'Average Dice Score on Validation Data: {avg_val_dice}')\n",
    "print(f'Average IoU on Validation Data: {avg_val_iou}')\n",
    "\n",
    "\n",
    "# Calculate metrics on validation data\n",
    "avg_test_dice, avg_test_iou = calculate_metrics(model, test_img_datagen, num_val_steps)\n",
    "\n",
    "print(f'\\nAverage Dice Score on Test Data: {avg_test_dice}')\n",
    "print(f'Average IoU on Test Data: {avg_test_iou}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "70b34d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "\n",
    "def calculate_metrics_per_class(model, data_generator, steps, num_classes):\n",
    "    dice_scores_per_class = {i: [] for i in range(num_classes)}\n",
    "    iou_scores_per_class = {i: [] for i in range(num_classes)}\n",
    "    \n",
    "    for _ in range(steps):\n",
    "        X_val, Y_val = next(data_generator)\n",
    "        predictions = model.predict(X_val)\n",
    "        \n",
    "        # Calculate dice score and IoU for each class in each sample\n",
    "#         for i in range(len(X_val)):\n",
    "#             p = Y_val[i]\n",
    "#             q = predictions[i]\n",
    "        for class_idx in range(num_classes):\n",
    "            y_true_class = Y_val[:, :, :, :, class_idx]\n",
    "            y_pred_class = predictions[:, :, :, :, class_idx]\n",
    "\n",
    "            dice = dice_coef(y_true_class, y_pred_class)\n",
    "            i = iou(y_true_class, y_pred_class)\n",
    "\n",
    "            dice_scores_per_class[class_idx].append(dice)\n",
    "            iou_scores_per_class[class_idx].append(i)\n",
    "    \n",
    "    # Calculate average scores for each class\n",
    "    avg_dice_per_class = {i: sum(scores) / len(scores) for i, scores in dice_scores_per_class.items()}\n",
    "    avg_iou_per_class = {i: sum(scores) / len(scores) for i, scores in iou_scores_per_class.items()}\n",
    "    \n",
    "    return avg_dice_per_class, avg_iou_per_class\n",
    "\n",
    "# Assuming you want to evaluate on a certain number of steps/batches\n",
    "num_steps = 10  # Change this to suit your testing preference\n",
    "num_classes = 4  # Change this to the number of classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e57449bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 0 - Average Dice Score on Validation Data: 0.9968904256820679\n",
      "Class 0 - Average IoU on Validation Data: 0.9938055872917175\n",
      "Class 1 - Average Dice Score on Validation Data: 0.7460901737213135\n",
      "Class 1 - Average IoU on Validation Data: 0.6343157887458801\n",
      "Class 2 - Average Dice Score on Validation Data: 0.7934149503707886\n",
      "Class 2 - Average IoU on Validation Data: 0.6796221733093262\n",
      "Class 3 - Average Dice Score on Validation Data: 0.8403255343437195\n",
      "Class 3 - Average IoU on Validation Data: 0.7266498804092407\n",
      "Mean Dice Score on Validation Data: 0.8441802859306335\n",
      "Mean IoU on Validation Data: 0.7585983276367188\n"
     ]
    }
   ],
   "source": [
    "# Calculate metrics on validation data for each class\n",
    "avg_val_dice_per_class, avg_val_iou_per_class = calculate_metrics_per_class(model, val_img_datagen, num_steps, num_classes)\n",
    "\n",
    "# Print the results for each class\n",
    "for class_idx in range(num_classes):\n",
    "    avg_dice = avg_val_dice_per_class[class_idx]\n",
    "    avg_iou = avg_val_iou_per_class[class_idx]\n",
    "    print(f'Class {class_idx} - Average Dice Score on Validation Data: {avg_dice}')\n",
    "    print(f'Class {class_idx} - Average IoU on Validation Data: {avg_iou}')\n",
    "\n",
    "# Calculate mean across all classes\n",
    "mean_dice = np.mean(list(avg_val_dice_per_class.values()))\n",
    "mean_iou = np.mean(list(avg_val_iou_per_class.values()))\n",
    "\n",
    "print(f\"Mean Dice Score on Validation Data: {mean_dice}\")\n",
    "print(f\"Mean IoU on Validation Data: {mean_iou}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e278b963",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 0 - Average Dice Score on Test Data: 0.9974530935287476\n",
      "Class 0 - Average IoU on Test Data: 0.9949232339859009\n",
      "Class 1 - Average Dice Score on Test Data: 0.6932392716407776\n",
      "Class 1 - Average IoU on Test Data: 0.5565176010131836\n",
      "Class 2 - Average Dice Score on Test Data: 0.8495780229568481\n",
      "Class 2 - Average IoU on Test Data: 0.7463930249214172\n",
      "Class 3 - Average Dice Score on Test Data: 0.8167829513549805\n",
      "Class 3 - Average IoU on Test Data: 0.6957644820213318\n",
      "Mean Dice Score on Test Data: 0.8392633199691772\n",
      "Mean IoU on Test Data: 0.7483996152877808\n"
     ]
    }
   ],
   "source": [
    "# Calculate metrics on validation data for each class\n",
    "avg_test_dice_per_class, avg_test_iou_per_class = calculate_metrics_per_class(model, test_img_datagen, num_steps, num_classes)\n",
    "\n",
    "\n",
    "\n",
    "# Print the results for each class\n",
    "for class_idx in range(num_classes):\n",
    "    avg_dice = avg_test_dice_per_class[class_idx]\n",
    "    avg_iou = avg_test_iou_per_class[class_idx]\n",
    "    print(f'Class {class_idx} - Average Dice Score on Test Data: {avg_dice}')\n",
    "    print(f'Class {class_idx} - Average IoU on Test Data: {avg_iou}')\n",
    "\n",
    "# Calculate mean across all classes\n",
    "mean_dice = np.mean(list(avg_test_dice_per_class.values()))\n",
    "mean_iou = np.mean(list(avg_test_iou_per_class.values()))\n",
    "\n",
    "print(f\"Mean Dice Score on Test Data: {mean_dice}\")\n",
    "print(f\"Mean IoU on Test Data: {mean_iou}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2962914a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
